
\section{Introduzione}

\subsection{Panoramica dell'algoritmo K-Nearest Neighbors (KNN)}
L'algoritmo K-Nearest Neighbors (KNN) rappresenta un pilastro fondamentale 
nell'ambito dell'apprendimento automatico supervisionato, apprezzato per la 
sua semplicità concettuale e la sua efficacia in una vasta gamma di applicazioni. 
La sua filosofia si basa sul principio intuitivo che oggetti simili tendono a 
raggrupparsi nello stesso spazio delle caratteristiche. Questo approccio non 
parametrico permette a KNN di adattarsi a strutture dati complesse e 
a relazioni non lineari, senza fare assunzioni rigide sulla distribuzione dei dati.

\subsection{Funzionamento di KNN}

KNN opera determinando le etichette di classificazione o i valori di 
regressione per un nuovo punto, basandosi sulla vicinanza ai punti di addestramento. 
Il parametro chiave di KNN è $K$, che rappresenta il numero di "vicini" più prossimi 
da considerare durante la fase di predizione. Quando un nuovo dato deve essere 
classificato o valutato, KNN calcola la distanza (come vedremo, esistono varie tipologie di distanze) 
tra il dato da classificare, o valutare, e tutti i punti di addestramento;
quindi seleziona i $K$ punti più vicini. La classe o 
il valore di regressione del nuovo dato è determinato dalla classe maggiormente rappresentata
o dalla media dei valori nei punti vicini, rispettivamente.

\subsection{Potenzialità di KNN}

KNN trova applicazione in numerosi settori grazie alla sua flessibilità e facilità di 
implementazione. Nei sistemi di raccomandazione, ad esempio, può suggerire prodotti o 
contenuti simili a quelli preferiti dall'utente sulla base dei gusti di altri utenti simili 
(vicini). Nell'analisi di immagini e nel riconoscimento di pattern, KNN può classificare 
nuove immagini confrontandole con esempi già noti. In ambito medico, può supportare la diagnosi 
confrontando i sintomi del paziente con casi storici simili.

Oltre ai settori menzionati, KNN può essere utilizzato in molti altri contesti. 
Nell'analisi di frodi, per esempio, KNN può identificare transazioni sospette 
confrontandole con transazioni conosciute come fraudolente. Nel settore del marketing, 
può segmentare i clienti in gruppi simili per creare campagne pubblicitarie mirate. 
Anche in ambito finanziario, KNN può essere utilizzato per prevedere l'andamento di 
titoli azionari o per valutare il rischio di credito basandosi su dati storici.

\subsection{Caratteristiche e Limitazioni}

Il KNN è noto per la sua semplicità e il principio della distanza su cui basare le decisioni 
di classificazione o regressione. Tuttavia, nella pratica, 
il KNN presenta diverse sfide significative.

Innanzitutto, KNN utilizza tutte le features in modo uguale, anche se alcune 
possono essere molto più predittive del target rispetto ad altre. Le distanze tra i 
punti vengono calcolate considerando tutte le features in modo uguale, utilizzando
metriche come la distanza Euclidea o Manhattan. Questo approccio, sebbene semplice, 
non è sempre il più efficace, poiché molte features possono essere irrilevanti per 
il target. Anche dopo aver effettuato una selezione delle features, la rilevanza 
delle stesse non sarà comunque uniforme.

Inoltre, le previsioni effettuate dai modelli KNN sono difficili da interpretare. 
Sebbene l'algoritmo sia comprensibile, capire le previsioni può essere complicato. È
possibile elencare i $K$ vicini più prossimi per un record, il che fornisce alcune 
indicazioni sulla previsione fatta per quel record, ma è difficile capire perché un 
determinato insieme di record sia considerato il più simile, soprattutto quando ci sono 
molte features in gioco.

Il KNN è anche sensibile al rumore nei dati e alla presenza di features non rilevanti, 
che possono influenzare negativamente le previsioni. Gestire grandi dataset con KNN può 
risultare computazionalmente oneroso, poiché richiede il calcolo delle distanze tra il nuovo 
dato e tutti i punti di addestramento. Questo problema diventa ancora più complesso quando 
si tratta di dataset con molte dimensioni (features).

Infine, quando i dati utilizzati da KNN sono altamente non strutturati, tipicamente non 
sono utili per comprendere la natura della relazione tra le features e il risultato 
della classe o della regressione. Questo limita ulteriormente l'interpretabilità del modello e 
la sua efficacia in situazioni reali.

\subsection{Definizione e concetto di base}

\subsubsection{Dataset, feature e variabile target}

Un dataset in Machine Learning è una collezione di dati organizzati in un formato strutturato. Ogni riga del dataset rappresenta un'osservazione, mentre ogni colonna rappresenta una caratteristica (feature) o la variabile target (etichetta). Le feature sono attributi che descrivono le osservazioni e possono essere di diversi tipi, come numeriche, categoriche o binarie. La variabile target è ciò che vogliamo predire utilizzando le feature.

Ad esempio, in un dataset per la predizione del prezzo delle case, le feature potrebbero includere la superficie della casa, il numero di camere, la posizione, l'anno di costruzione, ecc. La variabile target sarebbe il prezzo della casa.

\subsubsection{Problema di classificazione}

Consideriamo un esempio reale: la diagnosi precoce di malattie cardiache. Questo è un problema non banale che richiede l'uso del Machine Learning per essere risolto efficacemente. Il dataset potrebbe includere pazienti con varie caratteristiche cliniche misurate durante esami medici. Le feature potrebbero includere età, genere, pressione sanguigna, livelli di colesterolo, frequenza cardiaca massima, risultati di elettrocardiogrammi, e altre misure cliniche rilevanti. La variabile target sarebbe una variabile binaria che indica la presenza o l'assenza di una malattia cardiaca.

L'algoritmo KNN può essere utilizzato per classificare un nuovo paziente come "a rischio" o "non a rischio" di malattia cardiaca basandosi sui dati storici di altri pazienti. Quando un nuovo paziente entra per una valutazione, KNN calcola le distanze tra le caratteristiche cliniche del nuovo paziente e quelle dei pazienti nel dataset di addestramento. Seleziona i \( K \) pazienti più simili (i vicini più prossimi) e determina la classe del nuovo paziente in base alla maggioranza delle classi dei vicini.

Per esempio, se \( K = 5 \) e tra i 5 pazienti più vicini al nuovo paziente 3 hanno una malattia cardiaca e 2 no, KNN predice che il nuovo paziente è "a rischio" di malattia cardiaca. Questa previsione può aiutare i medici a prendere decisioni informate riguardo ulteriori test o trattamenti, dimostrando l'importanza e l'utilità del Machine Learning in contesti medici critici.

\subsubsection{Problema di regressione}

Un esempio di problema di regressione risolvibile con l'algoritmo KNN è la previsione del valore di mercato delle proprietà immobiliari in una città. Questo problema richiede un approccio che si affida al Machine Learning per ottenere stime accurate e affidabili, data la moltitudine di fattori che influenzano i prezzi delle case.

Consideriamo un dataset che include informazioni dettagliate sulle proprietà immobiliari di una città. Le feature possono includere:

\begin{itemize}
    \item Superficie della proprietà (in metri quadrati)
    \item Numero di camere da letto
    \item Numero di bagni
    \item Anno di costruzione
    \item Distanza dai servizi principali (scuole, ospedali, trasporti pubblici)
    \item Valutazioni della qualità del quartiere
    \item Prezzi recenti delle proprietà vicine
\end{itemize}

La variabile target in questo caso è il prezzo di vendita della proprietà.

Quando si vuole stimare il valore di una nuova proprietà, l'algoritmo KNN calcola la distanza tra le caratteristiche della nuova proprietà e quelle delle proprietà nel dataset di addestramento. Utilizzando una metrica di distanza (come la distanza euclidea), KNN identifica i \( K \) immobili più simili. 

Ad esempio, se \( K = 5 \), KNN selezionerà le cinque proprietà più vicine alla nuova proprietà in termini di caratteristiche. Il prezzo stimato per la nuova proprietà sarà la media dei prezzi delle cinque proprietà più vicine.
Il prezzo stimato della nuova proprietà sarà dunque

\[
\hat{y} = \frac{1}{K} \sum_{i \in \mathcal{N}_K(\mathbf{x})} y_i
\]

Dove \( K \) è il numero di vicini considerati, \( \mathcal{N}_K(\mathbf{x}) \) rappresenta l'insieme dei \( K \) vicini più prossimi e \( y_i \) è il prezzo della proprietà $i$-esima.

Questo approccio di regressione basato su KNN è particolarmente utile perché tiene conto della località spaziale e delle caratteristiche specifiche delle proprietà immobiliari. Inoltre, permette di adattarsi a variazioni non lineari e complesse nei dati, che sono comuni nel mercato immobiliare. La previsione accurata dei prezzi immobiliari è fondamentale per acquirenti, venditori, agenti immobiliari e investitori, rendendo KNN uno strumento prezioso in questo contesto.

\subsection{Obiettivi dell'articolo}

L'obiettivo principale di questo articolo è fornire una comprensione completa e dettagliata 
dell'algoritmo K-Nearest Neighbors (KNN) attraverso un'analisi sia teorica che pratica.

In primo luogo, l'articolo presenta i fondamenti teorici di KNN, spiegando il funzionamento 
dell'algoritmo, le metriche di distanza utilizzate (come la distanza euclidea e la distanza di Manhattan) 
e l'importanza della scelta del parametro $K$. Verranno, inoltre, discusse le implicazioni di queste scelte 
sulla performance dell'algoritmo.

In secondo luogo, verranno esaminate le proprietà matematiche di KNN, inclusa la complessità 
computazionale e le sfide legate alla "maledizione della dimensionalità". Saranno analizzati i 
trade-off tra bias e varianza per comprendere come ottimizzare le prestazioni di questo algoritmo.

In terzo luogo, l'articolo fornirà un'analisi empirica, confrontando KNN con altri algoritmi 
di Machine Learning su dataset standard. Questo confronto aiuterà a evidenziare i punti di forza 
e le limitazioni di KNN in scenari pratici.

Infine, l'articolo esplora le tecniche di ottimizzazione e miglioramento di KNN, 
come la normalizzazione dei dati, l'uso di KNN pesato e l'implementazione di strutture 
dati efficienti come KD-Trees.